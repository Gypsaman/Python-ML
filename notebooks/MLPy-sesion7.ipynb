{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5cf0f8d",
   "metadata": {},
   "source": [
    "# MLPy — Sesión 7: Técnicas de Regularización\n",
    "\n",
    "En esta sesión vamos a **provocar overfitting** a propósito y luego aplicar técnicas de regularización para mejorar la **generalización**.\n",
    "\n",
    "**Mantendremos constantes** (en la medida de lo posible):\n",
    "- dataset (clasificación 2D)\n",
    "- esquema de entrenamiento\n",
    "- métrica principal (accuracy)\n",
    "- comparación por curvas train/val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e119c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e1633d",
   "metadata": {},
   "source": [
    "## 1) Dataset 2D con split train/val\n",
    "\n",
    "Usamos un dataset sintético que permite ver rápidamente:\n",
    "- separación en el plano (x0, x1)\n",
    "- overfitting cuando el modelo es demasiado flexible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d3087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset base (dos nubes)\n",
    "N = 1200\n",
    "x0 = torch.randn(N//2, 2) + torch.tensor([-2.0, 0.0])\n",
    "x1 = torch.randn(N//2, 2) + torch.tensor([ 2.0, 0.0])\n",
    "X = torch.cat([x0, x1], dim=0)\n",
    "y = torch.cat([torch.zeros(N//2), torch.ones(N//2)]).long()\n",
    "\n",
    "# Mezclar\n",
    "perm = torch.randperm(N)\n",
    "X, y = X[perm], y[perm]\n",
    "\n",
    "# Hacemos un train pequeño para inducir overfitting\n",
    "n_train = 160\n",
    "X_train, y_train = X[:n_train], y[:n_train]\n",
    "X_val,   y_val   = X[n_train:], y[n_train:]\n",
    "\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "val_ds   = TensorDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
    "\n",
    "X_train.shape, X_val.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4539dfdd",
   "metadata": {},
   "source": [
    "## 2) Funciones auxiliares (train/eval + curvas)\n",
    "\n",
    "Incluimos:\n",
    "- `train_one_epoch(...)`\n",
    "- `evaluate(...)`\n",
    "- utilidades para graficar curvas\n",
    "\n",
    "También creamos una función `fit(...)` que guarda historial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70fe5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    loss_sum, correct, n = 0.0, 0, 0\n",
    "    for xb, yb in loader:\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss_sum += loss.item() * xb.size(0)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == yb).sum().item()\n",
    "        n += xb.size(0)\n",
    "    return loss_sum / n, correct / n\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, l1_lambda=0.0, augment_noise_std=0.0):\n",
    "    model.train()\n",
    "    loss_sum, correct, n = 0.0, 0, 0\n",
    "\n",
    "    for xb, yb in loader:\n",
    "        # Data augmentation simple: ruido gaussiano en entrenamiento\n",
    "        if augment_noise_std > 0:\n",
    "            xb = xb + augment_noise_std * torch.randn_like(xb)\n",
    "\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "\n",
    "        # L1 manual (PyTorch optimizers no traen \"l1\" built-in)\n",
    "        if l1_lambda > 0:\n",
    "            l1_pen = 0.0\n",
    "            for p in model.parameters():\n",
    "                l1_pen = l1_pen + p.abs().sum()\n",
    "            loss = loss + l1_lambda * l1_pen\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_sum += loss.item() * xb.size(0)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == yb).sum().item()\n",
    "        n += xb.size(0)\n",
    "\n",
    "    return loss_sum / n, correct / n\n",
    "\n",
    "def plot_history(hist, title):\n",
    "    epochs = range(1, len(hist[\"train_loss\"]) + 1)\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, hist[\"train_loss\"], label=\"train loss\")\n",
    "    plt.plot(epochs, hist[\"val_loss\"], label=\"val loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, hist[\"train_acc\"], label=\"train acc\")\n",
    "    plt.plot(epochs, hist[\"val_acc\"], label=\"val acc\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def fit(model, train_loader, val_loader, optimizer, epochs=120,\n",
    "        l1_lambda=0.0, augment_noise_std=0.0,\n",
    "        early_stopping=False, patience=12):\n",
    "    hist = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "    best_val = float(\"inf\")\n",
    "    best_state = None\n",
    "    bad_epochs = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        tr_loss, tr_acc = train_one_epoch(\n",
    "            model, train_loader, optimizer,\n",
    "            l1_lambda=l1_lambda,\n",
    "            augment_noise_std=augment_noise_std\n",
    "        )\n",
    "        va_loss, va_acc = evaluate(model, val_loader)\n",
    "\n",
    "        hist[\"train_loss\"].append(tr_loss)\n",
    "        hist[\"train_acc\"].append(tr_acc)\n",
    "        hist[\"val_loss\"].append(va_loss)\n",
    "        hist[\"val_acc\"].append(va_acc)\n",
    "\n",
    "        # Early stopping por val loss\n",
    "        if early_stopping:\n",
    "            if va_loss < best_val - 1e-6:\n",
    "                best_val = va_loss\n",
    "                best_state = {k: v.detach().clone() for k, v in model.state_dict().items()}\n",
    "                bad_epochs = 0\n",
    "            else:\n",
    "                bad_epochs += 1\n",
    "                if bad_epochs >= patience:\n",
    "                    break\n",
    "\n",
    "    if early_stopping and best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return hist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbeb96d",
   "metadata": {},
   "source": [
    "## 3) Un modelo \"grande\" para inducir overfitting\n",
    "\n",
    "Con pocos datos de entrenamiento, una red con muchas unidades puede memorizar.\n",
    "\n",
    "Incluimos una variante con dropout (probabilidad configurable).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f794043",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigMLP(nn.Module):\n",
    "    def __init__(self, dropout_p=0.0):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.Linear(2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "        ]\n",
    "        if dropout_p > 0:\n",
    "            # insert dropout between blocks (simple strategy)\n",
    "            layers = [\n",
    "                nn.Linear(2, 128), nn.ReLU(), nn.Dropout(dropout_p),\n",
    "                nn.Linear(128, 128), nn.ReLU(), nn.Dropout(dropout_p),\n",
    "                nn.Linear(128, 128), nn.ReLU(), nn.Dropout(dropout_p),\n",
    "            ]\n",
    "        layers.append(nn.Linear(128, 2))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def run_experiment(name, model, optimizer, **fit_kwargs):\n",
    "    hist = fit(model, train_loader, val_loader, optimizer, **fit_kwargs)\n",
    "    val_loss, val_acc = evaluate(model, val_loader)\n",
    "    print(f\"{name} | val_loss={val_loss:.4f} | val_acc={val_acc:.4f} | epochs={len(hist['train_loss'])}\")\n",
    "    plot_history(hist, title=name)\n",
    "    return hist, (val_loss, val_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779c8f1e",
   "metadata": {},
   "source": [
    "## 4) Baseline: sin regularización (espera overfitting)\n",
    "\n",
    "Entrenamos sin L1/L2, sin dropout, sin early stopping, sin augmentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437e1eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base = BigMLP(dropout_p=0.0)\n",
    "opt_base = torch.optim.SGD(model_base.parameters(), lr=0.08)\n",
    "\n",
    "hist_base, metrics_base = run_experiment(\n",
    "    \"Baseline (sin regularización)\",\n",
    "    model_base, opt_base,\n",
    "    epochs=160\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8072c677",
   "metadata": {},
   "source": [
    "## 5) L2 (weight decay)\n",
    "\n",
    "Implementación práctica: `weight_decay` en el optimizador.\n",
    "\n",
    "Interpretación: penaliza \\(\\|W\\|_2^2\\) y empuja pesos más pequeños.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b24e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l2 = BigMLP(dropout_p=0.0)\n",
    "opt_l2 = torch.optim.SGD(model_l2.parameters(), lr=0.08, weight_decay=1e-3)\n",
    "\n",
    "hist_l2, metrics_l2 = run_experiment(\n",
    "    \"L2 (weight_decay=1e-3)\",\n",
    "    model_l2, opt_l2,\n",
    "    epochs=160\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7adeb9",
   "metadata": {},
   "source": [
    "## 6) L1 (penalización manual)\n",
    "\n",
    "Aquí implementamos L1 agregando \\(\\lambda \\sum |w|\\) al loss.\n",
    "\n",
    "Nota: L1 puede requerir tuning más cuidadoso que L2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccb0c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l1 = BigMLP(dropout_p=0.0)\n",
    "opt_l1 = torch.optim.SGD(model_l1.parameters(), lr=0.08)\n",
    "\n",
    "hist_l1, metrics_l1 = run_experiment(\n",
    "    \"L1 (lambda=1e-5)\",\n",
    "    model_l1, opt_l1,\n",
    "    epochs=160,\n",
    "    l1_lambda=1e-5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1d01dd",
   "metadata": {},
   "source": [
    "## 7) Dropout\n",
    "\n",
    "Dropout introduce ruido estructurado en la red (apaga neuronas al azar).\n",
    "\n",
    "Regla práctica:\n",
    "- comenzar con p=0.1–0.3 en MLPs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4f6de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_do = BigMLP(dropout_p=0.25)\n",
    "opt_do = torch.optim.SGD(model_do.parameters(), lr=0.08)\n",
    "\n",
    "hist_do, metrics_do = run_experiment(\n",
    "    \"Dropout (p=0.25)\",\n",
    "    model_do, opt_do,\n",
    "    epochs=160\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc566c6",
   "metadata": {},
   "source": [
    "## 8) Early stopping\n",
    "\n",
    "Entrenamos muchos epochs, pero detenemos cuando la validación deja de mejorar.\n",
    "\n",
    "Ventaja: suele ser muy efectivo y barato.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a687196",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_es = BigMLP(dropout_p=0.0)\n",
    "opt_es = torch.optim.SGD(model_es.parameters(), lr=0.08)\n",
    "\n",
    "hist_es, metrics_es = run_experiment(\n",
    "    \"Early stopping (patience=12)\",\n",
    "    model_es, opt_es,\n",
    "    epochs=250,\n",
    "    early_stopping=True,\n",
    "    patience=12\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb0a908",
   "metadata": {},
   "source": [
    "## 9) Data augmentation simple: ruido en inputs\n",
    "\n",
    "En este dataset toy, usamos **ruido gaussiano en entrenamiento** como augmentación.\n",
    "\n",
    "Objetivo: que el modelo sea menos sensible a variaciones pequeñas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec78e2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_aug = BigMLP(dropout_p=0.0)\n",
    "opt_aug = torch.optim.SGD(model_aug.parameters(), lr=0.08)\n",
    "\n",
    "hist_aug, metrics_aug = run_experiment(\n",
    "    \"Augmentación (ruido std=0.15)\",\n",
    "    model_aug, opt_aug,\n",
    "    epochs=160,\n",
    "    augment_noise_std=0.15\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293cf9d3",
   "metadata": {},
   "source": [
    "## 10) Resumen rápido de resultados\n",
    "\n",
    "Comparamos métricas finales de validación.\n",
    "\n",
    "**Nota:** por el azar (inicialización/mini-batches), los números pueden variar, pero las tendencias deberían mantenerse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49393b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [\n",
    "    (\"Baseline\",) + metrics_base,\n",
    "    (\"L2\",) + metrics_l2,\n",
    "    (\"L1\",) + metrics_l1,\n",
    "    (\"Dropout\",) + metrics_do,\n",
    "    (\"Early stopping\",) + metrics_es,\n",
    "    (\"Augmentación\",) + metrics_aug,\n",
    "]\n",
    "\n",
    "for name, vloss, vacc in results:\n",
    "    print(f\"{name:15s}  val_loss={vloss:.4f}  val_acc={vacc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbca3ad",
   "metadata": {},
   "source": [
    "## 11) Preguntas de reflexión\n",
    "\n",
    "1. ¿Qué técnica te dio la mejor mejora en validación con el menor costo?\n",
    "2. ¿Qué técnica fue más sensible a hiperparámetros (por ejemplo \\(\\lambda\\))?\n",
    "3. ¿Qué pasa si combinamos técnicas (p.ej. L2 + early stopping + dropout)?\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
