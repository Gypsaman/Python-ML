{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7146279",
   "metadata": {},
   "source": [
    "# Sesión 1 — Puente: de Conceptos a Código (ML con Python / PyTorch)\n",
    "\n",
    "**Instructor:** Cesar Garcia  \n",
    "\n",
    "Esta sesión conecta el **curso conceptual de Machine Learning** (pérdida, gradientes, optimización, generalización)\n",
    "con **código real en PyTorch**.\n",
    "\n",
    "El objetivo no es memorizar APIs, sino **entender cómo se ve el aprendizaje en código**.\n",
    "\n",
    "---\n",
    "## Mapa mental del entrenamiento (recordatorio)\n",
    "\n",
    "1. **Forward** — predicción  \n",
    "2. **Loss** — medición del error  \n",
    "3. **Backward** — cálculo del gradiente  \n",
    "4. **Step** — actualización de parámetros  \n",
    "5. Repetir (batches y épocas)\n",
    "\n",
    "En esta notebook verás este ciclo **literalmente implementado**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c76f3d",
   "metadata": {},
   "source": [
    "## Concepto → objeto en PyTorch\n",
    "\n",
    "| Concepto | En PyTorch |\n",
    "|---|---|\n",
    "| Datos $(x, y)$ | `Dataset`, `DataLoader` |\n",
    "| Modelo | `nn.Module` |\n",
    "| Forward | `model(x)` |\n",
    "| Función de pérdida | `nn.MSELoss`, `nn.CrossEntropyLoss` |\n",
    "| Gradiente | `loss.backward()` |\n",
    "| Optimización | `torch.optim` |\n",
    "| Learning rate | `lr` |\n",
    "| Batch / iteración | `for xb, yb in loader:` |\n",
    "| Época | `for epoch in range(E):` |\n",
    "| Regularización | `weight_decay`, `Dropout` |\n",
    "| Evaluación | `model.eval()`, `torch.no_grad()` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3f553c",
   "metadata": {},
   "source": [
    "**Idea clave:**  \n",
    "NumPy calcula números.  \n",
    "PyTorch calcula números **y cómo cambian esos números**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ea9e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar disponibilidad de PyTorch (Colab usualmente ya lo incluye)\n",
    "\n",
    "import importlib, sys, subprocess\n",
    "\n",
    "def asegurar_torch():\n",
    "    spec = importlib.util.find_spec(\"torch\")\n",
    "    if spec is None:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"torch\", \"torchvision\", \"torchaudio\"])\n",
    "    else:\n",
    "        print(\"PyTorch ya está instalado.\")\n",
    "\n",
    "asegurar_torch()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "print(\"versión de torch:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9cbc62",
   "metadata": {},
   "source": [
    "## Dataset sintético (clasificación binaria)\n",
    "\n",
    "Usamos un dataset pequeño para concentrarnos en el **ciclo de entrenamiento**, no en ingeniería de datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2fc107",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "N = 400\n",
    "x0 = torch.randn(N//2, 2) + torch.tensor([-2.0, 0.0])\n",
    "x1 = torch.randn(N//2, 2) + torch.tensor([ 2.0, 0.0])\n",
    "\n",
    "X = torch.cat([x0, x1], dim=0)\n",
    "y = torch.cat([torch.zeros(N//2), torch.ones(N//2)]).long()\n",
    "\n",
    "ds = TensorDataset(X, y)\n",
    "train_loader = DataLoader(ds, batch_size=32, shuffle=True)\n",
    "\n",
    "X.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaf9b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "plt.scatter(x0[:, 0], x0[:, 1], label=\"Class 0\", alpha=0.7)\n",
    "plt.scatter(x1[:, 0], x1[:, 1], label=\"Class 1\", alpha=0.7)\n",
    "\n",
    "plt.xlabel(\"x₁\")\n",
    "plt.ylabel(\"x₂\")\n",
    "plt.legend()\n",
    "plt.title(\"Synthetic Binary Classification Dataset\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33635951",
   "metadata": {},
   "source": [
    "## Red neuronal mínima (MLP)\n",
    "\n",
    "**Correspondencia conceptual:**\n",
    "- Capas lineales → multiplicación matricial + sesgo\n",
    "- ReLU → no linealidad\n",
    "- Salida → *logits* (para clasificación)\n",
    "\n",
    "No confundir *logits* con logits de probabilidad: $\\ln{\\frac{p}{1-p}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f74096",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = MLP()\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e34bf3",
   "metadata": {},
   "source": [
    "## Función de pérdida y optimizador\n",
    "\n",
    "- Pérdida: `CrossEntropyLoss` (espera *logits*, no softmax)\n",
    "- Optimizador: `Adam`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f70a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "criterion, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d481cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1209598f",
   "metadata": {},
   "source": [
    "## Training loop — el puente completo\n",
    "\n",
    "Aquí aparece el ciclo de aprendizaje completo visto en el curso conceptual.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a45caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "E = 30\n",
    "for epoch in range(E):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)      # forward\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()         # backward\n",
    "        optimizer.step()        # actualización\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"época {epoch+1:02d} | pérdida {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01193bb5",
   "metadata": {},
   "source": [
    "## Evaluación mínima\n",
    "\n",
    "Esta evaluación usa los mismos datos de entrenamiento (optimista por diseño).\n",
    "La separación train/validación se verá en las siguientes sesiones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ad9ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(X)\n",
    "    pred = logits.argmax(dim=1)\n",
    "    acc = (pred == y).float().mean().item()\n",
    "\n",
    "acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05af870",
   "metadata": {},
   "source": [
    "## Orientación: arreglos NumPy vs tensores PyTorch\n",
    "\n",
    "Una pregunta común es:\n",
    "\n",
    "> *“¿PyTorch es solo NumPy con otra sintaxis?”*\n",
    "\n",
    "**No.** La diferencia clave es la **diferenciación automática**.\n",
    "\n",
    "### Comparación conceptual\n",
    "\n",
    "| Característica | NumPy | PyTorch |\n",
    "|---|---|---|\n",
    "| Objetivo principal | Cálculo numérico | Cálculo diferenciable |\n",
    "| Objeto central | `ndarray` | `Tensor` |\n",
    "| Gradientes | ❌ | ✅ |\n",
    "| Backpropagation | ❌ | ✅ |\n",
    "| Soporte GPU | ❌ | ✅ |\n",
    "| Entrenamiento ML | ❌ | ✅ |\n",
    "\n",
    "PyTorch existe porque **el aprendizaje automático necesita gradientes**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fec5eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ilustración mínima: por qué existe PyTorch\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Arreglo NumPy\n",
    "x_np = np.array([1.0, 2.0, 3.0])\n",
    "print(\"Suma NumPy:\", x_np.sum())\n",
    "\n",
    "# Tensor PyTorch con seguimiento de gradientes\n",
    "x_t = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y_t = x_t.sum()\n",
    "y_t.backward()\n",
    "\n",
    "print(\"Suma PyTorch:\", y_t.item())\n",
    "print(\"Gradiente:\", x_t.grad)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "ML_Sesion1_Puente_Conceptos_a_Codigo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
