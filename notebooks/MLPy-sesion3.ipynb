{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c3013af",
   "metadata": {},
   "source": [
    "# MLPy — Sesión 3: Red neuronal desde cero (sin `nn.Module`)\n",
    "\n",
    "**Objetivo:** implementar una red pequeña (2 capas) con PyTorch usando:\n",
    "- parámetros explícitos (`W1`, `b1`, `W2`, `b2`)\n",
    "- forward pass manual\n",
    "- pérdida (cross-entropy) como escalar\n",
    "- backward con autograd\n",
    "- actualización manual tipo SGD\n",
    "\n",
    "**Restricción:** no usar `torch.nn.Module` ni `torch.optim`.\n",
    "\n",
    "> Pregunta guía: ¿puedes explicar cada línea del entrenamiento con conceptos de álgebra lineal y gradientes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21c7570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed0cc14",
   "metadata": {},
   "source": [
    "## 1) Dataset sintético en 2D\n",
    "\n",
    "Creamos dos nubes de puntos (clase 0 y clase 1) para visualizar y entrenar fácilmente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2208cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 400\n",
    "x0 = torch.randn(N//2, 2) + torch.tensor([-2.0, 0.0])\n",
    "x1 = torch.randn(N//2, 2) + torch.tensor([ 2.0, 0.0])\n",
    "\n",
    "X = torch.cat([x0, x1], dim=0)\n",
    "y = torch.cat([torch.zeros(N//2), torch.ones(N//2)]).long()\n",
    "\n",
    "ds = TensorDataset(X, y)\n",
    "train_loader = DataLoader(ds, batch_size=32, shuffle=True)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4c7948",
   "metadata": {},
   "source": [
    "## 2) Definir parámetros (pesos y sesgos) explícitamente\n",
    "\n",
    "Red de 2 capas:\n",
    "- Entrada: 2\n",
    "- Oculta: 16\n",
    "- Salida: 2 (logits para 2 clases)\n",
    "\n",
    "**Nota:** `requires_grad=True` le dice a Autograd que debe calcular gradientes para esos tensores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592f65bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_in = 2\n",
    "h = 16\n",
    "d_out = 2\n",
    "\n",
    "# Inicialización simple (pequeña) para estabilidad\n",
    "W1 = 0.1 * torch.randn(d_in, h, requires_grad=True)\n",
    "b1 = torch.zeros(h, requires_grad=True)\n",
    "\n",
    "W2 = 0.1 * torch.randn(h, d_out, requires_grad=True)\n",
    "b2 = torch.zeros(d_out, requires_grad=True)\n",
    "\n",
    "W1.shape, b1.shape, W2.shape, b2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2176a95c",
   "metadata": {},
   "source": [
    "## 3) Forward pass manual\n",
    "\n",
    "- Capa 1: `H = ReLU(X @ W1 + b1)`\n",
    "- Capa 2: `logits = H @ W2 + b2`\n",
    "\n",
    "Trabajamos con **logits** y usamos `cross_entropy` directamente (incluye softmax internamente).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f2268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X):\n",
    "    z1 = X @ W1 + b1          # (batch, h)\n",
    "    h1 = torch.relu(z1)       # (batch, h)\n",
    "    logits = h1 @ W2 + b2     # (batch, 2)\n",
    "    return logits\n",
    "\n",
    "# Prueba rápida\n",
    "logits = forward(X[:5])\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec1b5c5",
   "metadata": {},
   "source": [
    "## 4) Pérdida (cross-entropy)\n",
    "\n",
    "`F.cross_entropy(logits, y)` devuelve un **escalar**.\n",
    "Ese escalar es el “punto de partida” para que Autograd calcule gradientes de todos los parámetros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb0e53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.cross_entropy(forward(X[:32]), y[:32])\n",
    "loss, loss.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b876032",
   "metadata": {},
   "source": [
    "## 5) Un paso de entrenamiento: backward + update manual (SGD)\n",
    "\n",
    "Pasos:\n",
    "1. Forward → logits\n",
    "2. Loss → escalar\n",
    "3. `loss.backward()` llena `W1.grad`, `b1.grad`, ...\n",
    "4. Actualización manual con learning rate\n",
    "5. Reiniciar gradientes a cero\n",
    "\n",
    "**Importante:** los gradientes se acumulan; por eso se limpian cada iteración.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb546f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "\n",
    "def zero_grads():\n",
    "    for p in (W1, b1, W2, b2):\n",
    "        if p.grad is not None:\n",
    "            p.grad.zero_()\n",
    "\n",
    "# Un mini-batch\n",
    "xb, yb = next(iter(train_loader))\n",
    "\n",
    "zero_grads()\n",
    "logits = forward(xb)\n",
    "loss = F.cross_entropy(logits, yb)\n",
    "loss.backward()\n",
    "\n",
    "# Magnitudes de gradiente (diagnóstico rápido)\n",
    "(W1.grad.abs().mean().item(), W2.grad.abs().mean().item(), loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d513ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"requires_grad:\", W1.requires_grad)\n",
    "print(\"is_leaf:\", W1.is_leaf)\n",
    "print(\"grad_fn:\", W1.grad_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f371166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for p in (W1, b1, W2, b2):\n",
    "        p -= lr * p.grad\n",
    "\n",
    "# Si no limpiamos, el siguiente backward acumularía gradientes\n",
    "zero_grads()\n",
    "\n",
    "\"updated!\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0a6b64",
   "metadata": {},
   "source": [
    "## 6) Entrenamiento completo (varias épocas)\n",
    "\n",
    "Entrenamos por unas épocas y registramos la pérdida promedio.\n",
    "\n",
    "Luego evaluamos accuracy sobre el mismo dataset (por simplicidad, aquí no usamos validación todavía).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba89844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(X, y):\n",
    "    with torch.no_grad():\n",
    "        logits = forward(X)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        return (preds == y).float().mean().item()\n",
    "\n",
    "epochs = 30\n",
    "lr = 0.1\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    running = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        zero_grads()\n",
    "        logits = forward(xb)\n",
    "        loss = F.cross_entropy(logits, yb)\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for p in (W1, b1, W2, b2):\n",
    "                p -= lr * p.grad\n",
    "\n",
    "        running += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    avg_loss = running / n_batches\n",
    "    loss_history.append(avg_loss)\n",
    "\n",
    "    if epoch in {1, 5, 10, 20, 30}:\n",
    "        print(f\"epoch {epoch:>2} | loss={avg_loss:.4f} | acc={accuracy(X,y):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38db879f",
   "metadata": {},
   "source": [
    "## 7) Visualizar la curva de pérdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c430e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average training loss\")\n",
    "plt.title(\"Training loss (manual SGD)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca5cbfb",
   "metadata": {},
   "source": [
    "## 8) (Opcional) Cross-entropy “a mano” (para entender)\n",
    "\n",
    "`F.cross_entropy` combina `log_softmax` + negative log-likelihood.\n",
    "Verificamos que coincide (aprox.) con el cálculo manual.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c815b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_manual(logits, y):\n",
    "    log_probs = logits - logits.logsumexp(dim=1, keepdim=True)   # log_softmax\n",
    "    nll = -log_probs[torch.arange(y.shape[0]), y]                # pick correct class\n",
    "    return nll.mean()\n",
    "\n",
    "xb, yb = next(iter(train_loader))\n",
    "logits = forward(xb)\n",
    "\n",
    "ce_torch = F.cross_entropy(logits, yb)\n",
    "ce_manual = cross_entropy_manual(logits, yb)\n",
    "\n",
    "ce_torch.item(), ce_manual.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e4476e",
   "metadata": {},
   "source": [
    "## 9) Ejercicios\n",
    "\n",
    "1. **Activation swap**: cambia `ReLU` por `tanh` y compara la curva de pérdida.  \n",
    "2. **Hidden size**: prueba `h=4`, `h=64` y observa convergencia y accuracy.  \n",
    "3. **Learning rate pathology**:\n",
    "   - prueba `lr=1.0` (¿diverge?)\n",
    "   - prueba `lr=0.001` (¿aprende lento?)\n",
    "4. **Zero grads removal**: comenta `zero_grads()` y observa qué ocurre.\n",
    "\n",
    "> Meta: justificar el comportamiento con gradientes, escala y estabilidad numérica.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
