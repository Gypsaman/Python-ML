{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87324371",
   "metadata": {},
   "source": [
    "# Sesión 13 — Transformers I: Secuencias y Atención\n",
    "\n",
    "En esta sesión construiremos la **intuición** y la mecánica base de la atención:\n",
    "- Tokens → embeddings\n",
    "- Proyecciones **Q, K, V**\n",
    "- **Scaled dot-product attention**\n",
    "- Visualización de pesos de atención\n",
    "\n",
    "Objetivo: que puedas mirar una matriz de atención y explicar qué está ocurriendo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5288abd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8415256a",
   "metadata": {},
   "source": [
    "## 1) Secuencia de ejemplo\n",
    "\n",
    "Simulamos una secuencia de **N tokens**, cada uno con embedding de dimensión **d_model**.\n",
    "(Por ahora no hacemos tokenización real; nos enfocamos en el mecanismo.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd30062",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "N = 6\n",
    "d_model = 16\n",
    "X = torch.randn(N, d_model)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fb481a",
   "metadata": {},
   "source": [
    "## 2) Proyecciones Q, K, V\n",
    "\n",
    "Para cada token creamos tres proyecciones lineales:\n",
    "- **Q** (Query): lo que busco\n",
    "- **K** (Key): lo que ofrezco\n",
    "- **V** (Value): lo que transmito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f437a77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrices de proyección (como si fueran capas lineales sin bias)\n",
    "Wq = torch.randn(d_model, d_model)\n",
    "Wk = torch.randn(d_model, d_model)\n",
    "Wv = torch.randn(d_model, d_model)\n",
    "\n",
    "Q = X @ Wq\n",
    "K = X @ Wk\n",
    "V = X @ Wv\n",
    "\n",
    "Q.shape, K.shape, V.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef81665",
   "metadata": {},
   "source": [
    "## 3) Scaled dot-product attention\n",
    "\n",
    "Computamos similitudes Q·K y normalizamos con softmax:\n",
    "\n",
    "\\[\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897e7649",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_k = d_model\n",
    "scores = (Q @ K.T) / (d_k ** 0.5)   # (N x N)\n",
    "weights = F.softmax(scores, dim=-1)\n",
    "out = weights @ V                  # (N x d_model)\n",
    "\n",
    "scores.shape, weights.shape, out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceba2bd4",
   "metadata": {},
   "source": [
    "## 4) Visualización: matriz de atención\n",
    "\n",
    "Cada fila (query) muestra a qué posiciones (keys) presta atención ese token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0213c507",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(weights.detach().cpu())\n",
    "plt.colorbar()\n",
    "plt.title('Pesos de atención (self-attention)')\n",
    "plt.xlabel('Key index (posición atendida)')\n",
    "plt.ylabel('Query index (token que atiende)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334ead9f",
   "metadata": {},
   "source": [
    "## 5) Inspección rápida\n",
    "\n",
    "Veamos, por ejemplo, qué posiciones recibe más peso para el token en la posición 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154a8c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "w = weights[i].detach()\n",
    "top = torch.topk(w, k=3)\n",
    "top"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd88e18d",
   "metadata": {},
   "source": [
    "## 6) Variante opcional: máscara causal (look-ahead)\n",
    "\n",
    "En modelos autoregresivos (p. ej. GPT), el token i **no** puede mirar posiciones futuras j > i.\n",
    "Aplicamos una máscara triangular superior antes del softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1d08c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.triu(torch.ones(N, N), diagonal=1).bool()\n",
    "scores_masked = scores.clone()\n",
    "scores_masked[mask] = -1e9\n",
    "weights_causal = F.softmax(scores_masked, dim=-1)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(weights_causal.detach().cpu())\n",
    "plt.colorbar()\n",
    "plt.title('Pesos de atención con máscara causal')\n",
    "plt.xlabel('Key index')\n",
    "plt.ylabel('Query index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ec2c31",
   "metadata": {},
   "source": [
    "## Preguntas de repaso\n",
    "\n",
    "1. ¿Qué significa que un token tenga pesos de atención concentrados en una sola posición?\n",
    "2. ¿Por qué escalamos por \\sqrt{d_k}?\n",
    "3. ¿Qué cambia al introducir máscara causal?"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
