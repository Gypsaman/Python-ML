{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f746473",
   "metadata": {},
   "source": [
    "# MLPy — Sesión 6: Inicialización de Pesos y Funciones de Activación\n",
    "\n",
    "En esta sesión estudiamos cómo **activaciones** e **inicialización** afectan:\n",
    "\n",
    "- el flujo de activaciones (forward)\n",
    "- el flujo de gradientes (backward)\n",
    "- la estabilidad del entrenamiento\n",
    "\n",
    "Mantendremos constantes:\n",
    "- dataset\n",
    "- arquitectura base\n",
    "- optimizador\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879cc353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e5404e",
   "metadata": {},
   "source": [
    "## 1) Dataset simple (clasificación 2D)\n",
    "\n",
    "Usamos el mismo dataset sintético para aislar efectos de activación/inicialización.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432ec504",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 800\n",
    "x0 = torch.randn(N//2, 2) + torch.tensor([-2.0, 0.0])\n",
    "x1 = torch.randn(N//2, 2) + torch.tensor([ 2.0, 0.0])\n",
    "\n",
    "X = torch.cat([x0, x1], dim=0)\n",
    "y = torch.cat([torch.zeros(N//2), torch.ones(N//2)]).long()\n",
    "\n",
    "# Mezclar\n",
    "perm = torch.randperm(N)\n",
    "X, y = X[perm], y[perm]\n",
    "\n",
    "# Split entrenamiento / validación\n",
    "train_frac = 0.8\n",
    "n_train = int(N * train_frac)\n",
    "X_train, y_train = X[:n_train], y[:n_train]\n",
    "X_val,   y_val   = X[n_train:], y[n_train:]\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(TensorDataset(X_val, y_val), batch_size=32, shuffle=False)\n",
    "\n",
    "X_train.shape, X_val.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8985d358",
   "metadata": {},
   "source": [
    "## 2) Funciones auxiliares (train/eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0101843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def accuracy_from_logits(logits, y):\n",
    "    return (logits.argmax(dim=1) == y).float().mean().item()\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer):\n",
    "    model.train()\n",
    "    loss_sum, acc_sum, n = 0.0, 0.0, 0\n",
    "    for xb, yb in loader:\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_sum += loss.item()\n",
    "        acc_sum += accuracy_from_logits(logits, yb)\n",
    "        n += 1\n",
    "    return loss_sum / n, acc_sum / n\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_one_epoch(model, loader):\n",
    "    model.eval()\n",
    "    loss_sum, acc_sum, n = 0.0, 0.0, 0\n",
    "    for xb, yb in loader:\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss_sum += loss.item()\n",
    "        acc_sum += accuracy_from_logits(logits, yb)\n",
    "        n += 1\n",
    "    return loss_sum / n, acc_sum / n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd23c73",
   "metadata": {},
   "source": [
    "## 3) Modelo con activación configurable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211e7cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(activation: str):\n",
    "    activation = activation.lower().strip()\n",
    "    if activation == \"relu\":\n",
    "        act = nn.ReLU()\n",
    "    elif activation == \"tanh\":\n",
    "        act = nn.Tanh()\n",
    "    elif activation == \"sigmoid\":\n",
    "        act = nn.Sigmoid()\n",
    "    elif activation == \"leaky_relu\":\n",
    "        act = nn.LeakyReLU(0.01)\n",
    "    else:\n",
    "        raise ValueError(f\"Activación desconocida: {activation}\")\n",
    "\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(2, 64),\n",
    "        act,\n",
    "        nn.Linear(64, 64),\n",
    "        act,\n",
    "        nn.Linear(64, 2)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aab550",
   "metadata": {},
   "source": [
    "## 4) Inicialización de pesos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b902154a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(model, scheme: str):\n",
    "    scheme = scheme.lower().strip()\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            if scheme == \"xavier\":\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif scheme == \"he\":\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif scheme == \"default\":\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(f\"Inicialización desconocida: {scheme}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e9ee70",
   "metadata": {},
   "source": [
    "## 5) Experimento principal: activación × inicialización\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3fe976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(activation: str, init_scheme: str, epochs: int = 30, lr: float = 0.1, seed: int = 0):\n",
    "    torch.manual_seed(seed)\n",
    "    model = make_model(activation)\n",
    "    init_weights(model, init_scheme)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    hist = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer)\n",
    "        va_loss, va_acc = eval_one_epoch(model, val_loader)\n",
    "\n",
    "        hist[\"train_loss\"].append(tr_loss)\n",
    "        hist[\"val_loss\"].append(va_loss)\n",
    "        hist[\"train_acc\"].append(tr_acc)\n",
    "        hist[\"val_acc\"].append(va_acc)\n",
    "\n",
    "    return hist\n",
    "\n",
    "configs = [\n",
    "    (\"relu\", \"default\"),\n",
    "    (\"relu\", \"he\"),\n",
    "    (\"tanh\", \"xavier\"),\n",
    "    (\"sigmoid\", \"xavier\"),\n",
    "    (\"leaky_relu\", \"he\"),\n",
    "]\n",
    "\n",
    "results = {f\"{a}+{i}\": run_experiment(a, i) for a, i in configs}\n",
    "list(results.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768a9626",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for label, hist in results.items():\n",
    "    plt.plot(hist[\"val_loss\"], label=label)\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"Val loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a944756",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for label, hist in results.items():\n",
    "    plt.plot(hist[\"val_acc\"], label=label)\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"Val accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cca603",
   "metadata": {},
   "source": [
    "## 6) Diagnóstico interno: neuronas muertas (ReLU)\n",
    "\n",
    "Medimos el porcentaje de activaciones **exactamente 0** después de ReLU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a25f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def relu_zero_fraction(model: nn.Module, loader) -> float:\n",
    "    model.eval()\n",
    "    zeros, total = 0, 0\n",
    "\n",
    "    for xb, _ in loader:\n",
    "        h = xb\n",
    "        for layer in model:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                h = layer(h)\n",
    "            elif isinstance(layer, nn.ReLU):\n",
    "                h = layer(h)\n",
    "                zeros += (h == 0).sum().item()\n",
    "                total += h.numel()\n",
    "            else:\n",
    "                h = layer(h)\n",
    "\n",
    "    return zeros / total if total > 0 else 0.0\n",
    "\n",
    "relu_default = make_model(\"relu\")\n",
    "init_weights(relu_default, \"default\")\n",
    "\n",
    "relu_he = make_model(\"relu\")\n",
    "init_weights(relu_he, \"he\")\n",
    "\n",
    "relu_default_zero = relu_zero_fraction(relu_default, train_loader)\n",
    "relu_he_zero = relu_zero_fraction(relu_he, train_loader)\n",
    "\n",
    "relu_default_zero, relu_he_zero\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe3ca8e",
   "metadata": {},
   "source": [
    "## 7) (Opcional) Gradientes que desaparecen: norma de gradiente por capa\n",
    "\n",
    "Medimos normas de gradiente en una sola iteración para ver si el gradiente se atenúa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f7f81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_norms_one_batch(model: nn.Module, xb: torch.Tensor, yb: torch.Tensor):\n",
    "    model.train()\n",
    "    logits = model(xb)\n",
    "    loss = criterion(logits, yb)\n",
    "\n",
    "    # limpiar grads\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            p.grad.zero_()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    norms = []\n",
    "    for name, p in model.named_parameters():\n",
    "        if p.grad is not None:\n",
    "            norms.append((name, float(p.grad.norm())))\n",
    "    return float(loss.item()), norms\n",
    "\n",
    "xb, yb = next(iter(train_loader))\n",
    "\n",
    "m1 = make_model(\"relu\"); init_weights(m1, \"default\")\n",
    "m2 = make_model(\"relu\"); init_weights(m2, \"he\")\n",
    "\n",
    "loss1, norms1 = grad_norms_one_batch(m1, xb, yb)\n",
    "loss2, norms2 = grad_norms_one_batch(m2, xb, yb)\n",
    "\n",
    "loss1, loss2, norms1[:2], norms2[:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059bfabe",
   "metadata": {},
   "source": [
    "## 8) Preguntas de cierre\n",
    "\n",
    "1. ¿Qué combinación mostró entrenamiento más estable en validación?\n",
    "\n",
    "2. ¿Qué activaciones tienden a saturarse (y por qué eso afecta gradientes)?\n",
    "\n",
    "3. ¿Qué observas al comparar ReLU con inicialización default vs He?\n",
    "\n",
    "4. ¿Cambiar optimizador arreglaría un problema de inicialización? ¿por qué?\n",
    "\n",
    "5. Si aumentamos profundidad, ¿qué esperas que ocurra con sigmoid/tanh?\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
