{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b671313f",
   "metadata": {},
   "source": [
    "# MLPy — Sesión 5: Optimizadores y Dinámica del Aprendizaje (PyTorch)\n",
    "\n",
    "En esta sesión comparamos **SGD**, **Momentum**, **RMSProp** y **Adam** manteniendo constantes:\n",
    "\n",
    "- el **mismo dataset**\n",
    "- el **mismo modelo**\n",
    "- el **mismo training loop**\n",
    "- las **mismas métricas**\n",
    "\n",
    "> La meta: observar **dinámica** (curvas) y **estabilidad**, no solo el resultado final.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217b32a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cf7b64",
   "metadata": {},
   "source": [
    "## 1) Dataset (clasificación 2D) y splits\n",
    "\n",
    "Usamos el mismo estilo de dataset que en Sesión 4 para enfocarnos en los optimizadores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca424382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset sintético: dos clases separadas en 2D\n",
    "N = 800\n",
    "x0 = torch.randn(N//2, 2) + torch.tensor([-2.0, 0.0])\n",
    "x1 = torch.randn(N//2, 2) + torch.tensor([ 2.0, 0.0])\n",
    "\n",
    "X = torch.cat([x0, x1], dim=0)\n",
    "y = torch.cat([torch.zeros(N//2), torch.ones(N//2)]).long()\n",
    "\n",
    "# Mezclar\n",
    "perm = torch.randperm(N)\n",
    "X, y = X[perm], y[perm]\n",
    "\n",
    "# Split entrenamiento / validación\n",
    "train_frac = 0.8\n",
    "n_train = int(N * train_frac)\n",
    "X_train, y_train = X[:n_train], y[:n_train]\n",
    "X_val,   y_val   = X[n_train:], y[n_train:]\n",
    "\n",
    "X_train.shape, X_val.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4732250",
   "metadata": {},
   "source": [
    "## 2) Modelo base y funciones comunes\n",
    "\n",
    "Mantendremos la arquitectura fija para que el *único* cambio sea el optimizador.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81d10a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(seed: int = 0) -> nn.Module:\n",
    "    torch.manual_seed(seed)\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(2, 16),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16, 2)\n",
    "    )\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def accuracy_from_logits(logits: torch.Tensor, y_true: torch.Tensor) -> float:\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    return (preds == y_true).float().mean().item()\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss, total_acc, n_batches = 0.0, 0.0, 0\n",
    "\n",
    "    for xb, yb in loader:\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc  += accuracy_from_logits(logits, yb)\n",
    "        n_batches += 1\n",
    "\n",
    "    return total_loss / n_batches, total_acc / n_batches\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_one_epoch(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, total_acc, n_batches = 0.0, 0.0, 0\n",
    "\n",
    "    for xb, yb in loader:\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc  += accuracy_from_logits(logits, yb)\n",
    "        n_batches += 1\n",
    "\n",
    "    return total_loss / n_batches, total_acc / n_batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc424b4",
   "metadata": {},
   "source": [
    "## 3) Fábrica de optimizadores\n",
    "\n",
    "Usaremos configuraciones típicas, pero **sin afirmar que son universales**.\n",
    "\n",
    "- Momentum: `momentum=0.9`\n",
    "- RMSProp / Adam: suelen funcionar con `lr` más pequeño que SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7263db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer(name: str, model: nn.Module, lr: float):\n",
    "    name = name.lower().strip()\n",
    "    if name == \"sgd\":\n",
    "        return torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    if name == \"momentum\":\n",
    "        return torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    if name == \"rmsprop\":\n",
    "        return torch.optim.RMSprop(model.parameters(), lr=lr, alpha=0.99, eps=1e-8)\n",
    "    if name == \"adam\":\n",
    "        return torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-8)\n",
    "    raise ValueError(f\"Optimizador desconocido: {name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bdef9d",
   "metadata": {},
   "source": [
    "## 4) Runner: mismo experimento, distinto optimizador\n",
    "\n",
    "Registramos por época:\n",
    "\n",
    "- `train_loss`, `val_loss`\n",
    "- `train_acc`, `val_acc`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920d97cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(opt_name: str, lr: float, epochs: int = 40, batch_size: int = 32, seed: int = 0):\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(TensorDataset(X_val, y_val),     batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = make_model(seed=seed)\n",
    "    optimizer = make_optimizer(opt_name, model, lr=lr)\n",
    "\n",
    "    hist = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "        va_loss, va_acc = eval_one_epoch(model, val_loader, criterion)\n",
    "\n",
    "        hist[\"train_loss\"].append(tr_loss)\n",
    "        hist[\"val_loss\"].append(va_loss)\n",
    "        hist[\"train_acc\"].append(tr_acc)\n",
    "        hist[\"val_acc\"].append(va_acc)\n",
    "\n",
    "    return hist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0fb97d",
   "metadata": {},
   "source": [
    "## 5) Comparación rápida (curvas) con hiperparámetros razonables\n",
    "\n",
    "Ejemplo típico:\n",
    "\n",
    "- SGD: `lr=0.1`\n",
    "- Momentum: `lr=0.1`\n",
    "- RMSProp: `lr=0.01`\n",
    "- Adam: `lr=0.01`\n",
    "\n",
    "La meta es comparar **dinámica**, no “ganadores absolutos”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f71c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = {\n",
    "    \"SGD (lr=0.1)\":       (\"sgd\", 0.1),\n",
    "    \"Momentum (lr=0.1)\":  (\"momentum\", 0.1),\n",
    "    \"RMSProp (lr=0.01)\":  (\"rmsprop\", 0.01),\n",
    "    \"Adam (lr=0.01)\":     (\"adam\", 0.01),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for label, (opt, lr) in experiments.items():\n",
    "    results[label] = run_experiment(opt, lr=lr, epochs=40, batch_size=32, seed=0)\n",
    "\n",
    "for label, hist in results.items():\n",
    "    print(f\"{label:18s} | val_loss={hist['val_loss'][-1]:.4f} | val_acc={hist['val_acc'][-1]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a885824d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for label, hist in results.items():\n",
    "    plt.plot(hist[\"train_loss\"], label=label)\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"Train loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ad3d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for label, hist in results.items():\n",
    "    plt.plot(hist[\"val_loss\"], label=label)\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"Val loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2705c839",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for label, hist in results.items():\n",
    "    plt.plot(hist[\"val_acc\"], label=label)\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"Val accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3049a8c",
   "metadata": {},
   "source": [
    "## 6) Sensibilidad al learning rate\n",
    "\n",
    "Probamos varios `lr` para **SGD** y **Adam** para comparar sensibilidad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e10287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep_lrs(opt_name: str, lrs, epochs: int = 25, batch_size: int = 32):\n",
    "    curves = {}\n",
    "    for lr in lrs:\n",
    "        hist = run_experiment(opt_name, lr=lr, epochs=epochs, batch_size=batch_size, seed=0)\n",
    "        curves[lr] = hist[\"val_loss\"]\n",
    "    return curves\n",
    "\n",
    "sgd_lrs  = [0.01, 0.05, 0.1, 0.2]\n",
    "adam_lrs = [0.001, 0.003, 0.01, 0.03]\n",
    "\n",
    "sgd_curves  = sweep_lrs(\"sgd\",  sgd_lrs,  epochs=25)\n",
    "adam_curves = sweep_lrs(\"adam\", adam_lrs, epochs=25)\n",
    "\n",
    "len(sgd_curves), len(adam_curves)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9476223",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for lr, curve in sgd_curves.items():\n",
    "    plt.plot(curve, label=f\"SGD lr={lr}\")\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"Val loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2b2d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for lr, curve in adam_curves.items():\n",
    "    plt.plot(curve, label=f\"Adam lr={lr}\")\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"Val loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e257560b",
   "metadata": {},
   "source": [
    "## 7) Mini-experimento geométrico: valle alargado (intuición)\n",
    "\n",
    "Función cuadrática mal condicionada:\n",
    "\n",
    "\\[ L(x, y) = a x^2 + b y^2 \\quad (a \\gg b) \\]\n",
    "\n",
    "Comparamos trayectorias con paso fijo vs Momentum (simulación didáctica).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2878f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quad_grad(xy, a=50.0, b=1.0):\n",
    "    x, y = float(xy[0]), float(xy[1])\n",
    "    return torch.tensor([2*a*x, 2*b*y], dtype=torch.float32)\n",
    "\n",
    "def gd_path(lr=0.02, steps=60, a=50.0, b=1.0, momentum=0.0):\n",
    "    xy = torch.tensor([2.0, 2.0], dtype=torch.float32)\n",
    "    v = torch.zeros_like(xy)\n",
    "    path = [xy.clone()]\n",
    "\n",
    "    for _ in range(steps):\n",
    "        g = quad_grad(xy, a=a, b=b)\n",
    "        if momentum > 0.0:\n",
    "            v = momentum * v + g\n",
    "            xy = xy - lr * v\n",
    "        else:\n",
    "            xy = xy - lr * g\n",
    "        path.append(xy.clone())\n",
    "    return torch.stack(path)\n",
    "\n",
    "path_gd  = gd_path(lr=0.02, steps=60, a=50.0, b=1.0, momentum=0.0)\n",
    "path_mom = gd_path(lr=0.02, steps=60, a=50.0, b=1.0, momentum=0.9)\n",
    "\n",
    "path_gd.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9106579",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(path_gd[:,0].numpy(),  path_gd[:,1].numpy(),  label=\"Paso fijo (sin momentum)\")\n",
    "plt.plot(path_mom[:,0].numpy(), path_mom[:,1].numpy(), label=\"Con momentum\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538b4e5e",
   "metadata": {},
   "source": [
    "## 8) Preguntas de cierre (para discusión)\n",
    "\n",
    "1. ¿Qué optimizador mostró curvas más estables? ¿por qué?\n",
    "\n",
    "2. ¿Cuál fue más sensible al learning rate: SGD o Adam?\n",
    "\n",
    "3. ¿Puedes identificar convergencia rápida pero poca mejora en validación?\n",
    "\n",
    "4. En el valle alargado, ¿qué cambió con Momentum: dirección, tamaño de paso, o ambos?\n",
    "\n",
    "5. ¿Qué criterio usarías para elegir optimizador en un proyecto real?\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
