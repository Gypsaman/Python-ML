{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30388e86",
   "metadata": {},
   "source": [
    "# Sesión 11 — CNN Fundamentals \n",
    "\n",
    "En esta sesión desarrollaremos intuición práctica de **convoluciones**:\n",
    "- qué hace un filtro\n",
    "- cómo aparecen los **feature maps**\n",
    "- qué efectos tienen **stride** y **padding**\n",
    "- cómo se conectan con una CNN real\n",
    "\n",
    "Pregunta guía: **¿Por qué un filtro aprendido puede reutilizarse en toda la imagen?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2408a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94ab6d2",
   "metadata": {},
   "source": [
    "## 1) Convolución “a mano” en 2D (intuición)\n",
    "\n",
    "Vamos a crear una imagen sintética con un borde, y aplicarle filtros clásicos:\n",
    "- borde vertical\n",
    "- borde horizontal\n",
    "\n",
    "Nota: aquí usamos filtros fijos para entender el mecanismo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ae8102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagen sintética: un \"borde\" vertical\n",
    "img = torch.zeros(1, 1, 32, 32)  # (N,C,H,W)\n",
    "img[:, :, :, :16] = 0.0\n",
    "img[:, :, :, 16:] = 1.0\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.imshow(img[0,0], cmap=\"gray\")\n",
    "plt.title(\"Imagen sintética (borde vertical)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2455a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir kernels (filtros) 3x3 estilo Sobel (aprox)\n",
    "k_vert = torch.tensor([[\n",
    "    [-1., 0., 1.],\n",
    "    [-2., 0., 2.],\n",
    "    [-1., 0., 1.],\n",
    "]], dtype=torch.float32)  # (1,3,3)\n",
    "\n",
    "k_horz = torch.tensor([[\n",
    "    [-1., -2., -1.],\n",
    "    [ 0.,  0.,  0.],\n",
    "    [ 1.,  2.,  1.],\n",
    "]], dtype=torch.float32)\n",
    "\n",
    "# Conv2d espera pesos con forma (out_channels, in_channels, kH, kW)\n",
    "W = torch.stack([k_vert, k_horz], dim=0).unsqueeze(1)  # (2,1,3,3)\n",
    "\n",
    "W.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97560494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar conv2d con padding para conservar tamaño\n",
    "out = F.conv2d(img, W, bias=None, stride=1, padding=1)  # (1,2,32,32)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
    "axes[0].imshow(img[0,0], cmap=\"gray\"); axes[0].set_title(\"Entrada\"); axes[0].axis(\"off\")\n",
    "axes[1].imshow(out[0,0].detach(), cmap=\"gray\"); axes[1].set_title(\"Filtro vertical\"); axes[1].axis(\"off\")\n",
    "axes[2].imshow(out[0,1].detach(), cmap=\"gray\"); axes[2].set_title(\"Filtro horizontal\"); axes[2].axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Pregunta: ¿por qué el filtro vertical responde fuerte cerca del borde vertical?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204c0826",
   "metadata": {},
   "source": [
    "## 2) Efecto de stride y padding\n",
    "\n",
    "- `stride` reduce resolución (submuestreo)\n",
    "- `padding` evita que la imagen “se encoja” demasiado y ayuda con bordes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd68c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar tamaños de salida\n",
    "out_s1_p0 = F.conv2d(img, W[:1], stride=1, padding=0)  # solo 1 filtro\n",
    "out_s1_p1 = F.conv2d(img, W[:1], stride=1, padding=1)\n",
    "out_s2_p1 = F.conv2d(img, W[:1], stride=2, padding=1)\n",
    "\n",
    "print(\"stride=1, padding=0:\", tuple(out_s1_p0.shape))\n",
    "print(\"stride=1, padding=1:\", tuple(out_s1_p1.shape))\n",
    "print(\"stride=2, padding=1:\", tuple(out_s2_p1.shape))\n",
    "\n",
    "# Pregunta: ¿qué ganas y qué pierdes al pasar de stride=1 a stride=2?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062b6505",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
    "axes[0].imshow(out_s1_p0[0,0], cmap=\"gray\"); axes[0].set_title(\"s=1, p=0\"); axes[0].axis(\"off\")\n",
    "axes[1].imshow(out_s1_p1[0,0], cmap=\"gray\"); axes[1].set_title(\"s=1, p=1\"); axes[1].axis(\"off\")\n",
    "axes[2].imshow(out_s2_p1[0,0], cmap=\"gray\"); axes[2].set_title(\"s=2, p=1\"); axes[2].axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b71c9e0",
   "metadata": {},
   "source": [
    "## 3) Una capa Conv2d real aprende los filtros\n",
    "\n",
    "Creamos una capa `nn.Conv2d` y observamos:\n",
    "- su forma de pesos\n",
    "- cómo produce múltiples mapas\n",
    "\n",
    "Todavía no entrenamos; esto es para ver estructura.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be525e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
    "conv.weight.shape, conv.bias.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f04efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar a una imagen MNIST para ver feature maps\n",
    "try:\n",
    "    from torchvision import datasets, transforms\n",
    "    from torch.utils.data import DataLoader\n",
    "    tfm = transforms.Compose([transforms.ToTensor()])\n",
    "    ds = datasets.MNIST(root=\"./data\", train=True, download=True, transform=tfm)\n",
    "    loader = DataLoader(ds, batch_size=1, shuffle=True)\n",
    "    x, y = next(iter(loader))\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"No pude cargar MNIST (torchvision). Error: {e}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    feats = conv(x)  # (1, 8, 28, 28)\n",
    "\n",
    "print(\"entrada:\", tuple(x.shape), \"feature maps:\", tuple(feats.shape), \"label:\", int(y.item()))\n",
    "\n",
    "# Mostrar algunos mapas\n",
    "fig, axes = plt.subplots(2, 4, figsize=(10, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(feats[0, i], cmap=\"gray\")\n",
    "    ax.set_title(f\"map {i}\")\n",
    "    ax.axis(\"off\")\n",
    "plt.suptitle(\"Feature maps (Conv2d sin entrenar)\")\n",
    "plt.show()\n",
    "\n",
    "# Pregunta: sin entrenar, ¿por qué algunos mapas ya muestran contrastes?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad55d6d",
   "metadata": {},
   "source": [
    "## 4) Mini-CNN (bloques)\n",
    "\n",
    "Patrón típico:\n",
    "- Conv → ReLU → Pool\n",
    "- repetir\n",
    "- Flatten → Linear\n",
    "\n",
    "En Session 12 construiremos una CNN completa para clasificación.\n",
    "Aquí solo verificamos shapes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef63bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini = nn.Sequential(\n",
    "    nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),  # 28x28 -> 14x14\n",
    "    nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),  # 14x14 -> 7x7\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    h = mini(x)  # x es (1,1,28,28)\n",
    "h.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5b3eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¿Cuántas características quedan después de los bloques conv?\n",
    "flatten_dim = h.numel()\n",
    "flatten_dim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f624504",
   "metadata": {},
   "source": [
    "## Cierre\n",
    "\n",
    "Hoy construimos intuición:\n",
    "\n",
    "- **convolución** = filtro que detecta patrones locales\n",
    "- **pesos compartidos** = el mismo patrón puede aparecer en cualquier lugar\n",
    "- **stride/padding/pooling** controlan resolución y robustez\n",
    "- apilar convs crea una **jerarquía de representaciones**\n",
    "\n",
    "Pregunta final: **¿Qué supuesto sobre imágenes hace que una CNN generalice mejor que un MLP?**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
