{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69621cfd",
   "metadata": {},
   "source": [
    "# Sesión 0 — Puente: de Conceptos a Código (ML con Python)\n",
    "\n",
    "**Autor:** Cesar Garcia  \n",
    "**Objetivo de la sesión:** conectar las ideas vistas en el curso conceptual (pérdida, gradiente, optimización, batches, generalización) con su equivalente directo en código usando **PyTorch**.\n",
    "\n",
    "---\n",
    "## Mapa mental del entrenamiento (recordatorio)\n",
    "\n",
    "1. **Forward** (predicción)  \n",
    "2. **Loss** (error)  \n",
    "3. **Backward** (gradiente)  \n",
    "4. **Step** (actualización)  \n",
    "5. Repetir (batches y épocas)\n",
    "\n",
    "En esta notebook vas a ver ese ciclo literalmente en un *training loop*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff34b70",
   "metadata": {},
   "source": [
    "## Concepto → objeto en PyTorch\n",
    "\n",
    "| Concepto | En PyTorch |\n",
    "|---|---|\n",
    "| Datos $x,y$ | `Dataset`, `DataLoader` |\n",
    "| Modelo (capas) | `torch.nn.Module`, `nn.Linear`, etc. |\n",
    "| Forward | `y_hat = model(x)` |\n",
    "| Función de pérdida | `nn.MSELoss()`, `nn.CrossEntropyLoss()` |\n",
    "| Gradiente | `loss.backward()` |\n",
    "| Optimización | `torch.optim.Adam(...)` |\n",
    "| Learning rate | `lr=...` |\n",
    "| Batch / iteración | `for xb, yb in loader:` |\n",
    "| Época | `for epoch in range(E):` |\n",
    "| Regularización | `weight_decay`, `Dropout`, early stopping |\n",
    "| Evaluación | `model.eval()`, `torch.no_grad()` |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d173663a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Opcional) Instalación de PyTorch si no está disponible en tu runtime\n",
    "# En Colab usualmente ya viene instalado. Este bloque verifica e instala solo si hace falta.\n",
    "\n",
    "import importlib, sys, subprocess\n",
    "\n",
    "def ensure_torch():\n",
    "    spec = importlib.util.find_spec(\"torch\")\n",
    "    if spec is None:\n",
    "        print(\"PyTorch no encontrado. Instalando...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"torch\", \"torchvision\", \"torchaudio\"])\n",
    "    else:\n",
    "        print(\"PyTorch ya está instalado.\")\n",
    "\n",
    "ensure_torch()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "print(\"torch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b095a34",
   "metadata": {},
   "source": [
    "## Dataset sintético (2 clases)\n",
    "\n",
    "Vamos a crear un dataset pequeño y fácil de entrenar. La intención no es “ganar accuracy real”, sino **ver el ciclo completo**:\n",
    "- datos → loader → modelo → loss → backward → optimizer step → evaluación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3236e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos sintéticos: dos nubes de puntos en 2D\n",
    "torch.manual_seed(0)\n",
    "\n",
    "N = 400\n",
    "x0 = torch.randn(N//2, 2) + torch.tensor([-2.0, 0.0])\n",
    "x1 = torch.randn(N//2, 2) + torch.tensor([ 2.0, 0.0])\n",
    "\n",
    "X = torch.cat([x0, x1], dim=0)                  # features\n",
    "y = torch.cat([torch.zeros(N//2), torch.ones(N//2)]).long()  # labels: 0/1\n",
    "\n",
    "# DataLoader (batching)\n",
    "ds = TensorDataset(X, y)\n",
    "train_loader = DataLoader(ds, batch_size=32, shuffle=True)\n",
    "\n",
    "X.shape, y.shape, next(iter(train_loader))[0].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c87e29",
   "metadata": {},
   "source": [
    "## Definir un modelo mínimo (MLP)\n",
    "\n",
    "**Decisión del programador (puente con lo conceptual):**\n",
    "- Arquitectura: `Linear → ReLU → Linear`\n",
    "- Salida: **logits** de tamaño 2 (para 2 clases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a624a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 2)  # logits para 2 clases\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = MLP()\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a412d8",
   "metadata": {},
   "source": [
    "## Elegir pérdida y optimizador\n",
    "\n",
    "**Regla práctica clave:**  \n",
    "`nn.CrossEntropyLoss()` espera **logits** (salida cruda). No apliques `softmax` antes.\n",
    "\n",
    "- Pérdida: `CrossEntropyLoss`\n",
    "- Optimizador: `Adam`\n",
    "- Learning rate: `1e-2` (intencionalmente algo alto para ver progreso rápido en notebook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a852ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "criterion, optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41828dee",
   "metadata": {},
   "source": [
    "## Training loop (el puente en 10 líneas)\n",
    "\n",
    "Aquí está el ciclo completo:\n",
    "\n",
    "- `logits = model(xb)` → **forward**  \n",
    "- `loss = criterion(logits, yb)` → **loss**  \n",
    "- `loss.backward()` → **gradiente**  \n",
    "- `optimizer.step()` → **actualización**  \n",
    "\n",
    "Nota: `optimizer.zero_grad()` evita acumular gradientes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c1b0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "E = 30\n",
    "for epoch in range(E):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)              # forward\n",
    "        loss = criterion(logits, yb)    # loss\n",
    "        loss.backward()                 # backward\n",
    "        optimizer.step()                # step\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"epoch {epoch+1:02d} | loss {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1ac952",
   "metadata": {},
   "source": [
    "## Evaluación mínima: accuracy\n",
    "\n",
    "**Concepto (Sesión 4):** evaluación en los mismos datos de entrenamiento suele ser optimista.  \n",
    "Aun así, sirve para verificar que el loop está funcionando.\n",
    "\n",
    "Buenas prácticas:\n",
    "- `model.eval()`\n",
    "- `torch.no_grad()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a6c868",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(X)\n",
    "    pred = logits.argmax(dim=1)\n",
    "    acc = (pred == y).float().mean().item()\n",
    "\n",
    "acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61988531",
   "metadata": {},
   "source": [
    "## Generalización (vista previa)\n",
    "\n",
    "En el curso, lo siguiente será separar `train/val/test` y medir correctamente generalización.\n",
    "\n",
    "Checklist de “generalización en código”:\n",
    "- Split de datos\n",
    "- Métricas en validación\n",
    "- Regularización si hay overfitting:\n",
    "  - `weight_decay` (L2)\n",
    "  - `Dropout`\n",
    "  - `Early stopping`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2146287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo rápido: cómo aparece L2 (weight decay) en PyTorch\n",
    "# (No lo usamos aquí, pero así se vería)\n",
    "adam_l2 = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "adam_l2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f0c842",
   "metadata": {},
   "source": [
    "## Cierre\n",
    "\n",
    "Si reconoces este patrón, ya entiendes cómo se ve el aprendizaje en Python:\n",
    "\n",
    "- Datos → modelo → pérdida → gradiente → actualización  \n",
    "- Repetición por batches y épocas  \n",
    "- Evaluación + regularización para generalización\n",
    "\n",
    "**Siguiente sesión recomendada:** Datos y preprocesamiento (scikit-learn + PyTorch).\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "ML_Sesion0_Puente_Conceptos_a_Codigo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
